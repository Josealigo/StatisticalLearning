{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# José Ligorría\n",
    "\n",
    "Se presenta resumen teorico de lo que son las máquinas de vectores de soporte, material que fue obtenido del trabajo de graduación : [Métodos selectos para categorizar crédito](https://ecfm.usac.edu.gt/sites/default/files/2018-11/Tesis%20Jose%20Ligorria_2.pdf) realizado por mipersona.\n",
    "\n",
    "# Máquinas de vectores de soporte\n",
    "\n",
    "## Descripción\n",
    "\n",
    "Las máquinas de vectores de soporte son algoritmos de clasificación y con ello predicción sobre una variable binaria o de solamente dos clases, como lo es el problema a trabajar en este trabajo. Básicamente este algoritmo busca el óptimo hiperplano con máximo _margen_ que separe a ambas clases en el espacio $ \\mathbb{R}^{n} $ donde pertenecen las variables numéricas que caracterizan a los elementos del conjunto a clasificar, al que llamaremos espacio de entradas. Entonces las máquinas de vectores de soporte buscan un método lineal para separar los datos que pertenecen al espacio de entradas y si no encuentra el separador lineal, proyecta los datos en un espacio de mayor dimensión donde sí se da la separación lineal, la proyección la realiza por técnicas _kernel_.\n",
    "\n",
    "### Margen SVM \n",
    "El **margen** de un hiperplano separador es la distancia más corta del hiperplano a un elemento de cada clase de las que separa a las que llamaremos **vectores de soporte**. Lo cual se puede visualizar en la figura \"Ejemplificación SVM\", donde el hiperplano separa a los elementos de las dos clases (círculos y estrellas), el margen es la distancia entre el hiperplano y los elementos más cercanos a éste, que son los vectores de soporte.\n",
    "\n",
    "<img src=\"EjemploSVM1.png\" alt=\"Alt text that describes the graphic\" title=\"Ejemplificación SVM\" />\n",
    "\n",
    "La función de clasificación o salida de una máquina de vectores de soporte en el caso más simple, es decir el caso lineal se describe a continuación:\n",
    "\n",
    "Suponiendo tenemos $ N $ observaciones $ (X_{i},Y_{i}) $ que tienen variables de valores numéricos o categóricos,  $ X_{i}=(x_{1}^{i},x_{2}^{i},...,x_{n}^{i}) $ donde $ x_{1},x_{2},...,x_{n} $ son variables numéricas que pertenecen a $ \\mathbb{R} $ y $ Y_{i} $ es la variable con dos posibles resultados convertidos a $ 1 $ y $ -1 $. Entonces la separación del espacio $ \\mathbb{R}^{n} $ se da por un funcional lineal $ f $ de tal forma que define los subespacios:\n",
    "\\\\\n",
    "\\\\\n",
    "$ A_ {1}:=\\lbrace Z \\in \\mathbb{R}^{n} | f (z) <b \\rbrace $ $ $ $ $ $ $ $ $ $  $ $ $ $ $ $ $ $ $ $ A_ {2}:= \\lbrace Z \\in \\mathbb{R}^{n} | f (z) > b \\rbrace $ \n",
    "\\\\\n",
    "\\\\\n",
    "Y como $ \\mathbb{R}^{n} $ es un espacio de Hilbert, el funcional tiene una representación de Riesz y $ f(X)=\\langle X, w \\rangle $ con  $ w= (w_ {1}, w_ {2},..., w_ {n}) $. Así la máquina de vectores de soporte tiene como aplicación de salida, para $ X $ en el conjunto de entradas, la siguiente función:\n",
    "#### Salida SVM caso lineal\n",
    "\n",
    "$U(X)= \\overset{n}{\\underset{i=1} \\sum} x_ {i} w_ {i}-b$\n",
    "\n",
    "Donde el hiperplano separador es aquél que cumple $ U=0 $, que denotaremos por $ L $ y los puntos más cercanos caen en el hiperplano $ U=\\pm 1 $. Entonces el margen es la distancia equivalente a esa unidad $ 1 $ de distancia entre los valores de las imágenes de $U$, que corresponde a la distancia entre los vectores de soporte y el hiperplano. Para ello consideremos que el vector $ w $ es perpendicular al plano ya que para $ z_ {1} $, $ z_ {2} \\in L$:\n",
    "\n",
    "$ \\langle z_{1}-z_{2},w \\rangle =\\langle z_{1},w \\rangle-\\langle z_{2},w \\rangle $ y como $ \\langle z_{1},w \\rangle+b=0 $ y $ \\langle z_{2},w \\rangle +b=0 $ entonces $ \\langle z_{1},w \\rangle -\\langle z_{2},w \\rangle =-b+b=0 $\n",
    "\n",
    "Entonces $ z_ {1} + \\dfrac{w}{\\Vert w \\Vert_{2}} $ es un elemento que está a $ 1 $ unidad (según la métrica usual en $ \\mathbb{R}^{n} $) del hiperplano separador, por lo que el valor de ese elemento evaluado en $ U $ está a una diferencia de:\n",
    "\n",
    "$u(z_{1} + \\dfrac{w}{\\Vert w \\Vert_{2} })= \\langle z_{1} + \\dfrac{w}{\\Vert w \\Vert_{2} },w \\rangle -b=\\langle z_{1},w \\rangle +\\langle \\dfrac{w}{\\Vert w \\Vert_{2}},w \\rangle -b=\\Vert w \\Vert_{2} $\n",
    "\n",
    "Así que los elementos que cumplen con $ U=\\pm 1 $ están a $ \\dfrac{1}{\\Vert w \\Vert_{2}} $ de distancia del hiperplano separador ya que la diferencia del valor de $ u $ con la distancia sobre $ w $ de dos vectores son cantidades proporcionales, por lo que el margen $ m $ está dado por:\n",
    "\\begin{equation}\\label{Margen}\n",
    "m=\\dfrac {2} {\\Vert w \\Vert_{2}}\n",
    "\\end{equation}\n",
    "\n",
    "Las máquinas de vectores de soporte buscan maximizar dicha expresión (o el cuadrado de la misma) para así tener el hiperplano que separe dejando la mayor distancia posible entre las clases, es decir el detalle de ellas radica en el problema de optimización siguiente:\n",
    "\n",
    "#### Optimización del margen para SVM\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{b,w}{\\textbf{mín}} \\frac{1}{2}  \\left\\{ \\Vert w \\Vert_{2} \\right\\} \\\\\n",
    "& \\text{s.a.} \\ \\ \\ \\ Y_ {i} (w \\cdot X_ {i} -b) \\geq 1, \\ \\ \\forall i = 1, 2, 3,..., N\n",
    "\\end{split}\n",
    "\n",
    "Aunque esta optimización se llevaría a cabo de una manera más eficiente con solamente los vectores de soporte, debido a que todavía no se ha entrenado a la máquina, no se puede saber cuáles serían vectores de soporte por lo que los datos $ X $ son cualesquiera para el conjunto de entrenamiento. Para resolver el problema se considera la solución por la Lagrangiana y con ello construir el problema dual obtenido al optimizar la expresión para los multiplicadores de Lagrange $ a $. Ya que en el óptimo global las condiciones de Kuhn-Tucker garantizan que el gradiente de la Lagrangiana es cero, y así convirtiendo el problema de optimización al problema dual por medio de la Lagrangiana tenemos:\n",
    "\n",
    "#### Optimización problema dual del margen para SVM KT\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{a}{\\text{mín}}  \\left\\{ \\frac{1}{2} \\overset{N}{\\underset{i=1} \\sum} \\overset{N}{\\underset{j=1} \\sum} a_{i} a_{j}Y_{i}Y_{j}(X_{j} \\cdot X_{i}) -\\overset{N}{\\underset{i=1} \\sum} a_{i} \\right\\}  \\\\\n",
    "& \\text{s.a.} \\ \\ \\ \\  \\overset{N}{\\underset{i=1} \\sum} a_{i}Y_{i}=0 , a_{i}\\geq 0 \\ \\ \\forall i=1,2,...,N\n",
    "\\end{split}\n",
    "\n",
    "Además en el procedimiento se obtiene que $ w= \\overset{N}{\\underset{i=1} \\sum}a_{i}Y_{i}X_{i} $ y para algún $ a_ {k}rlbrace0 $ se tiene que $ b=w \\cdot X_ {k} -y_ {k} $. El manipular el problema es útil ya que baja el tiempo computacional para encontrar $ w $ vía la solución del problema dual de optimización.\n",
    "\n",
    "Por otro lado, como no todos los conjuntos pueden ser linealmente separables entonces se modifican los argumentos originales a optimizar (ecuación \"Optimización del margen para SVM\") y en lugar de solamente optimizar el margen, se le agrega una penalización que a la vez permite a un elemento del conjunto de entrenamiento pertenecer al lugar que debería debido a su clasificación, ésta modificación es la siguiente: \n",
    "\n",
    "\n",
    "#### Optimización del margen para SVM modificada\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{b,w}{\\text{mín}} \\frac{1}{2} \\left\\{ \\Vert w \\Vert_{2} +\\gamma \\overset{N}{\\underset{i=1} \\sum} \\xi _ {i} \\right\\} \\\\ \n",
    "& \\text{s.a.} \\ \\ \\ \\ Y_ {i} (w \\cdot X_ {i} -b) \\geq 1 -\\xi_ {i}, \\ \\xi_{i} \\geq 0 \\ \\ \\forall i = 1, 2,3,..., N\n",
    "\\end{split}\n",
    "\n",
    "Donde $ \\gamma $ es la variable que penaliza la máquina de vectores de soporte al limitar el efecto de los vectores de soporte sobre la función de salida y $ \\xi _{i} $ son variables que corrigen el fallo del margen para cada observación en el conjunto de entrenamiento, es decir la variable que acerca a los elementos en el lugar donde deberían estar por su clasificación. Al realizar esta modificación, el problema dual queda similar solamente con el cambio de que ahora $ \\gamma \\geq a_ {i}\\geq 0 \\ \\ \\forall i = 1,2,..., N $.\n",
    "\n",
    "Así tenemos totalmente determinado el modo operacional de las máquinas de vectores de soporte para el caso en que se puede encontrar en el espacio de entradas un hiperplano separador, lo cual sucede pero no siempre, por lo que ya entendiendo el caso sencillo procedemos a analizar algo más general, que requiere el utilizar un mapeo para las entradas y así trasladarlos a un espacio de más dimensiones donde sí pueda ocurrir la separación. El resultado es similar con la salvedad de que cambia los vectores de entradas por su mapeo al espacio de más dimensiones, y ya con estas nuevas entradas se hace el mismo procedimiento que el ya realizado; el detalle de esto es que primero se debe decidir qué mapeo utilizar y luego calcular los mapeos del conjunto de entrenamiento para luego poder hacer los productos puntos con vectores de mayor dimensión, y así no presenta mayor complejidad en el razonamiento, sólo en el cómputo de los mapeos y los productos internos, lo cual si es considerable ya que encontrar la forma del mapeo no es sencillo. Por estas consideraciones las máquinas de vectores de soporte utilizan un procedimiento analítico para evitar utilizar dicho mapeo, el cual es el siguiente:\n",
    "\n",
    "Si $ \\Phi $ es el mapeo que manda al conjunto de entrenamiento (al que denotaremos como $ \\mathfrak{D}$) al espacio de mayor dimensión, entonces se considera definido de manera implícita por una función kernel, es decir, una función cuya entrada son los vectores del conjunto de entrenamiento y su salida es el producto interno de las imágenes de dichos puntos en el mapeo $ \\Phi $. Así si $ \\Phi: \\mathfrak{D} \\rightarrow \\mathfrak{H} $ con $ \\mathfrak{H} $ el espacio de mayor dimensión, entonces la función kernel $ k: \\mathfrak{D} \\times \\mathfrak{D} \\rightarrow \\mathbb{R} $  para $ X, X^ {\\prime} \\in \\mathfrak{D} $ está dada por:\n",
    "\n",
    "\\begin{equation}\\label{Función Kernel}\n",
    "k(X, X^ {\\prime}) :=\\langle \\Phi (X), \\Phi (X^ {\\prime}) \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "Esta técnica es usualmente llamada como el $ `` $ truco del kernel$ \" $ y ayuda computacionalmente hablando ya que es más sencillo que mapear explícitamente los vectores $ X, X^ {\\prime} $ en $ \\mathfrak{H} $. Lo que se debe de considerar ahora es escoger el kernel correcto o diseñarlo para así trabajar en espacios de gran dimensión de manera implícita y computacionalmente más sencilla, siendo la salida de la máquina la siguiente:\n",
    "\n",
    "\\begin{equation}\\label{Salida SVM caso no lineal}\n",
    "u(X)= \\Phi (X) \\cdot w-b\n",
    "\\end{equation}\n",
    "\n",
    "Entonces se plantea el mismo problema pero con condiciones diferentes, quedando así el sigiuente problema de optimización:\n",
    "\n",
    "\\begin{equation}\\label{Optimización del margen para SVM modificada no lineal para clasificación}\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{b,w}{\\text{mín}} \\frac{1}{2} \\left\\{ \\Vert w \\Vert_{2} +\\gamma \\overset{N}{\\underset{i=1} \\sum} \\xi _ {i} \\right\\} \\\\ \n",
    "& \\text{s.a.} \\ \\ \\ \\ Y_ {i} (w \\cdot \\Phi (X_ {i}) -b) \\geq 1 -\\xi_ {i}, \\ \\xi_{i} \\geq 0 \\ \\ \\forall i = 1,2, 3,..., N\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    " Donde el vector $ w $ y la solución final, es decir la función salida de la máquina de vectores de soporte, se encuentran de de manera parecida a el caso no lineal debido a que el método a aplicar aquí es el mismo, quedando así de la siguiente manera:\n",
    "\n",
    "\\begin{equation}\\label{Vector w para la solución para SVM general para clasificación}\n",
    " w= \\overset{N}{\\underset{i=1} \\sum}a_{i}Y_{i}\\Phi (X_ {i})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{Solución Salida SVM no lineal}\n",
    "u(X)=\\overset {N} {\\underset{i=1} \\sum} Y_ {i} a_ {i} k(X, X_ {i}) -b\n",
    "\\end{equation}\n",
    "\n",
    "Donde los valores $ a_ {i} $ $ \\forall $ $ i=1,2,..., N $ están dados por el problema de optimización:\n",
    "\n",
    "\\begin{equation}\\label{Optimización problema dual del margen para SVM modificada y no lineal KT}\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{a}{\\text{mín}}  \\left\\{ \\frac{1}{2} \\overset{N}{\\underset{i=1} \\sum} \\overset{N}{\\underset{j=1} \\sum} a_{i} a_{j}Y_{i}Y_{j} k(X_{j}, X_{i}) -\\overset{N}{\\underset{i=1} \\sum} a_{i} \\right\\} \\\\\n",
    "& \\text{s.a.} \\ \\ \\ \\  \\overset{N}{\\underset{i=1} \\sum} a_ {i} Y_ {i}=0, \\gamma \\geq a_ {i}\\geq 0 \\ \\ \\forall i=1,2,..., N\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "El cual es resuelto con optimización cuadrática, siendo un problema QP que tiene una complejidad cuasipolinomial, es decir que el tiempo que se utiliza en su solución crece más rápido que de manera polinomial pero más lento que de manera exponencial, dependiendo de la cantidad de variables.\n",
    "\n",
    "Ahora bien, si en lugar de clasificar lo que queremos es encontrar casos extraños en los datos, lo que se hace es de alguna forma con una máquina de vectores de soporte crear un tipo de esfera en la que los datos que caen afuera son los datos a considerar como extraños, lo cual se logra con el parámetro $ \\nu $ que modifica el volumen de la $ `` $esfera$ \" $ que deja afuera a los vectores extraños al establecer una cota superior para la fracción de extraños a encontrar y $ \\rho $ es un término que indica el error empírico del margen, en este caso la optimización de la función de salida de la máquina de vectores de soporte es:\n",
    "\n",
    "\\begin{equation}\\label{Optimización del margen para SVM modificada no lineal para búsqueda de extraños}\n",
    "\\begin{split}\n",
    "\\\n",
    "\\underset{b,w,\\rho}{\\text{mín}} \\frac{1}{2} \\left\\{ \\Vert w \\Vert_{2} +\\dfrac {1} {\\nu} \\ \\overset{N}{\\underset{i=1} \\sum} \\xi _ {i} -  \\rho \\right\\} \\\\ \n",
    "& \\text{s.a.} \\ \\ \\ \\ (w \\cdot \\Phi (X_ {i}) -b) \\geq \\rho -\\xi_ {i}, \\ \\xi_{i} \\geq 0 \\ \\ \\forall i = 1, 2,3,..., N\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Donde de igual manera se construye el problema dual y se encuentran los valores de los multiplicadores de Lagrange con la ayuda del truco del kernel.\n",
    "\n",
    "### Posibles funciones kernel\n",
    "\n",
    "Para escoger la función a usar como kernel se puede construir una en base a los datos y la experiencia adquirida por las máquinas de vectores de soporte o en el caso más general se puede escoger entre las siguientes:\n",
    "\n",
    "\n",
    "- El kernel lineal:\n",
    "\n",
    "$ k(X, X^ {\\prime}):= \\langle X, X^ {\\prime} \\rangle$\n",
    "- El kernel de base radial Gaussiana\n",
    "\n",
    "$ k(X, X^ {\\prime}):= e^ {- \\sigma \\Vert X-X^ {\\prime} \\Vert ^ {2}} $\n",
    "\n",
    "- El kernel polinomial\n",
    "\n",
    "$ k(X, X^ {\\prime}):= (\\text{escala} \\langle X, X^ {\\prime} \\rangle)+\\text{sesgo} )^{\\text{grado}}$\n",
    "\n",
    "- El kernel de tangente hiperbólica\n",
    "\n",
    "$ k(X, X^ {\\prime}):= \\text{tanh} (\\text{escala}\\langle X, X^ {\\prime} \\rangle+\\text{sesgo})$\n",
    "\n",
    "- El kernel de base radial Laplaciana\n",
    "\n",
    "$ k(X, X^ {\\prime}):= e^ {- \\sigma \\Vert X-X^ {\\prime} \\Vert} $\n",
    "\n",
    "\n",
    "\n",
    "Los kernel de base radial son usados generalmente cuando no se tiene un gran conocimiento previo sobre los datos, el lineal  cuando los datos son vectores dispersos, el polinomial cuando se trata de procesamiento de imágenes y el de tangente hiperbólica para el cálculo en redes neuronales.\n",
    "\n",
    "Para el entrenamiento de las máquinas de vectores de soporte las variables necesitan ser numéricas por lo que se debe hacer, al igual que para las redes neuronales, una conversión de las categorías, en el caso más sencillo y rápido de determinar por medio de banderas. Además también se consideran los conjuntos de datos para entrenamiento, para validación y para prueba. El primer conjunto para la creación de la función y optimización de la misma, el segundo para encontrar los parámetros óptimos del modelo y el de prueba, para dar el error o eficiencia de la máquina.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
