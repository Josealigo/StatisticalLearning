{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop y entrenamiento para XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es contruir por medio de una red neuronal con back propagation un ajuste a la función XOR, la cual tiene la siguiente forma y resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1 0\n"
     ]
    }
   ],
   "source": [
    "def operacion_XOR(x1,x2):\n",
    "    _and = x1+x2-1>0\n",
    "    _or = x1+x2>0\n",
    "    return int(_or-_and)\n",
    "\n",
    "print(operacion_XOR(0,0),operacion_XOR(0,1),operacion_XOR(1,0),operacion_XOR(1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimarla probamos primero con la función de error MSE. Y definimos la siguiente clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers = [3, 2, 2, 1], activations=['relu', 'relu','linear'],mu= 0, sigma = 0.1,loos = 'MSE',seed = 0): # Definimos la forma que creemos nos ayudará\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.loos = loos\n",
    "        self.seed = seed\n",
    "        for i in range(len(layers)-1): # Iniciamos los pesos y sesgos de manera random según una distribución normal.\n",
    "            np.random.seed(seed=self.seed+10*i)\n",
    "            self.weights.append(np.random.normal(self.mu, self.sigma, (layers[i+1], layers[i])))\n",
    "            #print('capa ', i, self.weights[-1])\n",
    "            np.random.seed(seed=self.seed+10*i+1)\n",
    "            self.biases.append(np.random.normal(self.mu, self.sigma, (layers[i+1], 1)))\n",
    "            #print('capa ', i, self.biases[-1])\n",
    "                \n",
    "    @staticmethod\n",
    "    def getActivationFunction(name): # Método que nos guarda las funciones de activación necesarias\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : 1/(1+np.exp(-x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "        \n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name): # Método que nos guarda las derivadas de las funciones de activación necesarias.\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1\n",
    "        \n",
    "    def error(self,y,y_hat,loos):\n",
    "        if loos == 'MSE':\n",
    "            return y-y_hat\n",
    "        if loos == 'Cross-entropy':\n",
    "            N = y_hat.shape[0]\n",
    "            ce = -(y * np.log(y_hat))\n",
    "            return ce\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        a = np.copy(x)\n",
    "        z_s = []\n",
    "        a_s = [a]\n",
    "        for i in range(len(self.weights)):\n",
    "            #print('Vamos en la capa ', i)\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            #print(self.weights[i],self.weights[i].shape)\n",
    "            #print(a,a.shape)\n",
    "            z_s.append(self.weights[i].dot(a) + self.biases[i]) \n",
    "            a = activation_function(z_s[-1])\n",
    "            a_s.append(a)\n",
    "        return (z_s, a_s)\n",
    "    \n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "        dw = []  # dC/dw\n",
    "        db = []  # dC/db\n",
    "        deltas = [None] * len(self.weights)  # delta = dC/dZa\n",
    "        deltas[-1] = ((self.error(y,a_s[-1],self.loos))*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1])) # Delta L\n",
    "        for i in reversed(range(len(deltas)-1)): # Calculamos los deltas para atrás\n",
    "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        batch_size = y.shape[0] \n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas] # guardamos las derivadas respecto a los sesgos\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)] # guardamos las derivadas respecto a los pesos\n",
    "        return dw, db          \n",
    "    \n",
    "    def train(self, x, y, epochs=100, lr = 0.01):\n",
    "        for e in range(epochs): \n",
    "            z_s, a_s = self.feedforward(x)\n",
    "            dw, db = self.backpropagation(y, z_s, a_s)\n",
    "            self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "            self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "            if (e+1)%np.floor(epochs/10) == 0:\n",
    "                print(\"Epoch {} with loss = {}\".format( e,np.linalg.norm(a_s[-1]-y)) )\n",
    "        print(\"Final epoch with loss = {}\".format(np.linalg.norm(a_s[-1]-y) )) \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los datasets a utilizar en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 0, 1],\n",
       "        [0, 0, 1, 1],\n",
       "        [1, 1, 1, 1]]),\n",
       " array([0., 1., 1., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [0,1]\n",
    "x2 = [0,1]\n",
    "dataset_x = np.array([[a,b,1] for a in x1 for b in x2]).reshape(3, -1,order='F')\n",
    "dataset_y_xor = np.zeros(len(x1)*len(x2))\n",
    "for i in range(len(dataset_y_xor)):\n",
    "    a = dataset_x[0][i]\n",
    "    b = dataset_x[1][i]\n",
    "    dataset_y_xor[i] = operacion_XOR(int(a),int(b))\n",
    "dataset_x,dataset_y_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 with loss = 1.2277822111553804\n",
      "Epoch 19 with loss = 1.1320224927552718\n",
      "Epoch 29 with loss = 1.0723555226990684\n",
      "Epoch 39 with loss = 1.0351642852826382\n",
      "Epoch 49 with loss = 1.0136027420347316\n",
      "Epoch 59 with loss = 1.003334180367543\n",
      "Epoch 69 with loss = 1.0013156177993405\n",
      "Epoch 79 with loss = 1.0053096634729246\n",
      "Epoch 89 with loss = 1.013637645403523\n",
      "Epoch 99 with loss = 1.0250304951551408\n",
      "Final epoch with loss = 1.0250304951551408\n"
     ]
    }
   ],
   "source": [
    "nn1 = NeuralNetwork(loos = 'Cross-entropy')\n",
    "\n",
    "nn1.train(dataset_x, dataset_y_xor, epochs=100,  lr = 0.01)\n",
    "_, a_s = nn1.feedforward(dataset_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60625811, 0.61475495, 0.60882006, 0.61992377]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_s[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.18261837,  0.04630825,  0.11037946],\n",
       "        [ 0.22716406,  0.18986958, -0.09153927]]),\n",
       " array([[ 0.16533424,  0.07606243],\n",
       "        [-0.15454003, -0.00083838]]),\n",
       " array([[0.23390158, 0.0195865 ]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
